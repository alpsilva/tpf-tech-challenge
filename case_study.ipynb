{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esse caso de estudo, discutirei uma possível solução para o problema apresentado.\n",
    "\n",
    "Alguns pontos observados inicialmente, ao olhar para o formato de dado usado como exemplo:\n",
    "\n",
    "- O relatório/fonte de dados está armazenado no que assumo ser uma planilha de Excel/google sheets. Isso é um ponto positivo, pois a maior parte das ferramentas de importação de dados tem uma integração razoável com esse tipo de arquivo.\n",
    "- Considero a fonte de dados como tendo uma certa estrutura, mesmo que o formato não seja o mais ideal para análises de dados.\n",
    "- Temos alguns dados \"soltos\", que são relevantes àquela coleta de dados, mas não se encaixam em nenhuma tabela. Isso indica que precisaremos de uma classe de dados mais complexa, algo que armazene todos os detalhes presentes na metade superior do relatório, além das tabelas presentes na metade inferior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dadas essas observações, já é possível traçejar e implementar uma possível solução para o manuseio dos dados.\n",
    "\n",
    "Antes disso, porém, gostaria de destacar alguns pontos que irei \"assumir\" para esse estudo de caso, em nome da objetividade e clareza da solução.\n",
    "\n",
    "- Suposição 1: Todos as células presentes no relatório são fixas, isso é, entre um relatório e outro, não ocorrerá de uma descrição de dado e seu valor atrelado mudarem de lugar numa frequência que necessite de retrabalho constante.\n",
    "- Suposição 2: Todos os relatórios são iguais. Além das células não mudarem de lugar, todos os relatórios possuirão as mesmas células e os mesmos tipos de dados contidos nelas. Dessa forma, é possível implementar uma solução genérica que funcione em qualquer relatório enviado.\n",
    "- Suposição 3: Não é necessário separar as tabelas de movimentos/contagem de veículos. Pude notar por meio da análise prévia do relatório que existe uma separação entre os períodos de coleta (manhã, tarde e noite) e até entre os intervalos do mesmo período. O dado do intervalo em si é, claro, muito importante, mas é possível armazenar todos os períodos e intervalos em apenas uma estrutura de dados, sem impactar a capacidade de fazermos consultas a períodos e intervalos específicos. Para ser franco, é possível que tenha um impacto negativo na performance, pois teríamos tabelas maiores, mas é um custo ínfimo se comparado a praticidade e facilidade de implementação ao trabalharmos com apenas uma tabela.\n",
    "- Suposição 4: Cada conjunto de colunas [Horário] + [Auto, Bus, Cam., Moto, Bici. UVP] corresponde a apenas 1 movimento. Para estruturação no formato de tabela, faremos com que o movimento seja mais uma coluna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após todas as observações e suposições detalhadas, podemos seguir adiante com os detalhes da solução:\n",
    "\n",
    "1. O coração da solução serão os objetos do tipo dataframe, muito utilizados para representar dados colunares.\n",
    "2. Comumente, a biblioteca de python \"Pandas\" é utilizada para manuseio de dataframes, mas para esse exercício, utilizarei a biblioteca \"Polars\" que é mais nova (e por isso menos \"estável\" e atestada, porém tem performance muito superior se comparada à \"Pandas\". Fonte: https://h2oai.github.io/db-benchmark/). Além disso, para importação direta de datas, será necessário plugar a lib \"pyarrow\" ao \"Polars\"\n",
    "3. Como citado na etapa de observação, temos alguns dados que não cabem na visão de dataframe construída até aqui, como \"Ponto de Pesquisa\" ou \"Endereço\". É possível incorporar \"data da pesquisa\" na tabela, se for necessário, mas nesse ponto, não está clara a necessidade disso. Sendo assim, faz-se necessário uma classe que armazene os dados gerais e dataframe. Essa abordagem talvez não faça sentido num Python Notebook como esse, mas tem suas vantagens quando aplicada à produção de um projeto.\n",
    "4. Será necessário um script que extraia todos os dados relevantes da planilha e os use para compor os objetos estruturados criados em Python. Para esse fim, a biblioteca \"openpyxl\" será usada. Ela permite que cada célula individual de um arquivo .xlsx seja acessada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para começar, instalamos e importamos as dependências necessárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: polars in /home/alps2/.local/lib/python3.10/site-packages (0.14.11)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openpyxl in /home/alps2/.local/lib/python3.10/site-packages (3.0.10)\n",
      "Requirement already satisfied: et-xmlfile in /home/alps2/.local/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyarrow in /home/alps2/.local/lib/python3.10/site-packages (9.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/alps2/.local/lib/python3.10/site-packages (from pyarrow) (1.23.2)\n"
     ]
    }
   ],
   "source": [
    "# These extra bits are fail-safes to ensure the packages are being installed in\n",
    "# the jupyter kernel that is actually, currently, running\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install polars\n",
    "!{sys.executable} -m pip install openpyxl\n",
    "!{sys.executable} -m pip install pyarrow\n",
    "\n",
    "import polars as pl\n",
    "import openpyxl\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementação do extrator de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xlsx_extractor(file_path: str):\n",
    "    \"\"\"\n",
    "    Receives a .xlsx file path.\n",
    "    return the data extracted from the sheet.    \n",
    "    \"\"\"\n",
    "    def extract_hour_from_interval(date_interval: str):\n",
    "        \"\"\"\n",
    "        Receives a string in the format:\n",
    "        \"hh:mm às hh:mm\"\n",
    "        Returns the 2 hours in datetime.time format.\n",
    "        \"\"\"\n",
    "        start_time = date_interval[:5]\n",
    "        end_time = date_interval[9:]\n",
    "\n",
    "        start_time = datetime.strptime(start_time, '%H:%M').time()\n",
    "        end_time = datetime.strptime(end_time, '%H:%M').time()\n",
    "\n",
    "        return start_time, end_time\n",
    "\n",
    "    workbook = openpyxl.load_workbook(file_path)\n",
    "    sheet = workbook.active\n",
    "\n",
    "    # Getting general data\n",
    "\n",
    "    research_point = sheet['B2'].value\n",
    "    address = sheet['B3'].value\n",
    "    research_date = sheet['B6'].value\n",
    "    researh_date_weekday = sheet['C6'].value\n",
    "    period_rows = [5, 8, 13]\n",
    "\n",
    "    intervals_fhp = {}\n",
    "    for period_row in period_rows:\n",
    "        period = sheet.cell(row=period_row, column=1).value\n",
    "        period = period[9:]\n",
    "\n",
    "        row_range = 1\n",
    "        if period == \"Tarde\":\n",
    "            row_range = 3\n",
    "\n",
    "        for i in range(row_range):\n",
    "            index = i + 1\n",
    "            interval = sheet.cell(row=period_row+index, column=5)\n",
    "            interval_start, interval_end = extract_hour_from_interval(interval.value)\n",
    "            fhp = sheet.cell(row=period_row+index, column=7).value\n",
    "            key = (period, interval_start, interval_end)\n",
    "            intervals_fhp[key] = fhp\n",
    "\n",
    "    # Getting table data\n",
    "\n",
    "    table_data = {\n",
    "        \"interval_start\": [],\n",
    "        \"interval_end\": [],\n",
    "        \"movimento\": [],\n",
    "        \"Auto\": [],\n",
    "        \"Bus\": [],\n",
    "        \"Cam.\": [],\n",
    "        \"Moto\": [],\n",
    "        \"Bici.\": [],\n",
    "        \"UVP\": [],\n",
    "    }\n",
    "\n",
    "    # Iterate through rows and movimentos to get count data.\n",
    "\n",
    "    movimentos_columns = [2, 8]\n",
    "\n",
    "    for movimento_column in movimentos_columns:\n",
    "        movimento = sheet.cell(row=16, column=movimento_column).value\n",
    "\n",
    "        # getting interval separately\n",
    "        for row in sheet.iter_rows(min_row=19, min_col=1, max_row=22, max_col=1, values_only=True):\n",
    "            interval = row[0]\n",
    "            interval_start, interval_end = extract_hour_from_interval(interval)\n",
    "            table_data[\"interval_start\"].append(interval_start)\n",
    "            table_data[\"interval_end\"].append(interval_end)\n",
    "\n",
    "        min_col = movimento_column\n",
    "        max_col = min_col+5\n",
    "\n",
    "        for row in sheet.iter_rows(min_row=19, min_col=min_col, max_row=22, max_col=max_col, values_only=True):\n",
    "            table_data[\"movimento\"].append(int(movimento))\n",
    "            table_data[\"Auto\"].append(int(row[0]))\n",
    "            table_data[\"Bus\"].append(int(row[1]))\n",
    "            table_data[\"Cam.\"].append(int(row[2]))\n",
    "            table_data[\"Moto\"].append(int(row[3]))\n",
    "            table_data[\"Bici.\"].append(int(row[4]))\n",
    "            table_data[\"UVP\"].append(int(row[5]))\n",
    "\n",
    "    columns = [\n",
    "        (\"interval_start\", pl.Time),\n",
    "        (\"interval_end\", pl.Time),\n",
    "        (\"movimento\", pl.Int64),\n",
    "        (\"Auto\", pl.Int64),\n",
    "        (\"Bus\", pl.Int64),\n",
    "        (\"Cam.\", pl.Int64),\n",
    "        (\"Moto\", pl.Int64),\n",
    "        (\"Bici.\", pl.Int64),\n",
    "        (\"UVP\", pl.Float64),\n",
    "    ]\n",
    "    dataframe = pl.DataFrame(table_data, columns=columns)\n",
    "\n",
    "    return {\n",
    "        \"research_point\": research_point,\n",
    "        \"address\": address,\n",
    "        \"research_date\": research_date,\n",
    "        \"researh_date_weekday\": researh_date_weekday,\n",
    "        \"intervals_fhp\": intervals_fhp,\n",
    "        \"dataframe\": dataframe\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testando extrator e visualizando o objeto dataframe criado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "research_point : 4.0\n",
      "address : Rua tal\n",
      "research_date : 2022-08-01 00:00:00\n",
      "researh_date_weekday : Segunda\n",
      "intervals_fhp : {('Manhã', datetime.time(7, 30), datetime.time(8, 30)): 0.87, ('Tarde', datetime.time(12, 0), datetime.time(13, 0)): 0.6, ('Tarde', datetime.time(14, 30), datetime.time(15, 30)): 0.67, ('Tarde', datetime.time(17, 0), datetime.time(18, 0)): 0.62, ('Noite', datetime.time(20, 0), datetime.time(21, 0)): 0.71}\n",
      "shape: (5, 9)\n",
      "┌────────────────┬──────────────┬───────────┬──────┬─────┬──────┬──────┬───────┬───────┐\n",
      "│ interval_start ┆ interval_end ┆ movimento ┆ Auto ┆ ... ┆ Cam. ┆ Moto ┆ Bici. ┆ UVP   │\n",
      "│ ---            ┆ ---          ┆ ---       ┆ ---  ┆     ┆ ---  ┆ ---  ┆ ---   ┆ ---   │\n",
      "│ time           ┆ time         ┆ i64       ┆ i64  ┆     ┆ i64  ┆ i64  ┆ i64   ┆ f64   │\n",
      "╞════════════════╪══════════════╪═══════════╪══════╪═════╪══════╪══════╪═══════╪═══════╡\n",
      "│ 07:30:00       ┆ 07:45:00     ┆ 1         ┆ 131  ┆ ... ┆ 1    ┆ 16   ┆ 1     ┆ 154.0 │\n",
      "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "│ 07:45:00       ┆ 08:00:00     ┆ 1         ┆ 142  ┆ ... ┆ 0    ┆ 33   ┆ 0     ┆ 174.0 │\n",
      "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "│ 08:00:00       ┆ 08:15:00     ┆ 1         ┆ 118  ┆ ... ┆ 0    ┆ 17   ┆ 2     ┆ 155.0 │\n",
      "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "│ 08:15:00       ┆ 08:30:00     ┆ 1         ┆ 80   ┆ ... ┆ 2    ┆ 16   ┆ 0     ┆ 119.0 │\n",
      "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n",
      "│ 07:30:00       ┆ 07:45:00     ┆ 2         ┆ 40   ┆ ... ┆ 1    ┆ 12   ┆ 0     ┆ 46.0  │\n",
      "└────────────────┴──────────────┴───────────┴──────┴─────┴──────┴──────┴───────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./case_study_assets/TPF-test.xlsx\"\n",
    "extracted_data = xlsx_extractor(file_path)\n",
    "for key in extracted_data:\n",
    "    if key != \"dataframe\":\n",
    "        print(key, \":\", extracted_data[key])\n",
    "\n",
    "df = extracted_data['dataframe']\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
